<h1>DIT5411 Machine Learning Project</h1>
<h2>Forecasting Hong Kong Daily Grass Minimum Temperature using RNN, LSTM, and BiLSTM</h2>

This project develops and compares three deep learning models—Simple RNN, LSTM, and Bidirectional LSTM—to forecast Hong Kong’s daily grass minimum temperature using historical data from the Hong Kong Observatory (HKO). The workflow follows the official DIT5411 Machine Learning Project requirements, including data preprocessing, sequence generation, model development, evaluation, visualisation, and documentation.

<h2>Table of Contents</h2>

- Project Overview  
- Dataset and Preprocessing  
- Project Structure  
- Model Architectures  
- Training and Evaluation  
- Results  
- Visualisations  
- How to Run the Project  
- Discussion and Future Work  
- Dataset Reference  

<h2>Project Overview</h2>

Grass minimum temperature is an important meteorological indicator reflecting radiative cooling and seasonal variation. The dataset is smooth, highly seasonal, and predictable, making it suitable for sequential deep learning models.

This project includes:

- Training models using data from 1980–2024  
- Testing on unseen data from 2025-01-01 to 2025-10-30  
- Comparing RNN, LSTM, and BiLSTM performance  
- Analysing prediction errors  
- Documenting the entire workflow in GitHub  

<h2>Dataset and Preprocessing</h2>

Source: Hong Kong Observatory Daily Grass Minimum Temperature dataset.

Main preprocessing steps:

- Load the raw CSV (`daily_HKO_GMT_ALL.csv`).  
- Parse date fields and construct a continuous daily time index.  
- Handle missing values using time-based interpolation.  
- Restrict the usable range from 1980-01-01 to 2025-10-30.  
- Save the cleaned series as `processed_HKO_GMT_ALL.csv`.  

For modelling, the data are transformed into supervised learning samples using a sliding window of 45 days to predict the next day’s grass minimum temperature.

<h2>Project Structure</h2>

```
Project/
├── daily_HKO_GMT_ALL.csv          # Raw HKO dataset
├── processed_HKO_GMT_ALL.csv      # Cleaned dataset
├── data_preprocessing.py          # Cleaning, interpolation, datetime handling
├── sequence_generator.py          # Sliding-window (45 days) sequence generation
├── models_rnn_lstm.py             # RNN, LSTM, BiLSTM model definitions
├── train_and_evaluate.py          # Training, evaluation, visualisation
├── hyperparameter_tuning.py       # Optional: window size experiments (30/45/60)
├── models/
│   ├── rnn_model.h5
│   ├── lstm_model.h5
│   └── bilstm_model.h5
└── figures/
    ├── actual_vs_predicted.png
    └── error_distribution.png
```

<h2>Model Architectures</h2>

All models use a sequence-to-one setup with a 45-day input sequence predicting the next day’s temperature.

**1. Simple RNN (Baseline)**  
- SimpleRNN(100, return_sequences=True)  
- SimpleRNN(100)  
- Dense(1)

**2. LSTM (Advanced Model)**  
- LSTM(100, return_sequences=True)  
- Dropout(0.2)  
- LSTM(100)  
- Dropout(0.2)  
- Dense(1)

**3. Bidirectional LSTM (Optional Extension)**  
- Bidirectional(LSTM(100, return_sequences=True))  
- Dropout(0.2)  
- Bidirectional(LSTM(100))  
- Dropout(0.2)  
- Dense(1)

Optimizer: Adam  
Loss: MSE  
Metric: MAE  

<h2>Training and Evaluation</h2>

Training settings:
- Epochs: 80  
- Batch size: 32  
- Validation split: 0.1  
- Train/test split based on dates  
- Window size: 45  

Models are evaluated on the 2025 dataset.

<h2>Results (2025 Test Set)</h2>

| Model | MAE | RMSE |
|-------|------|--------|
| RNN | 1.4411 | 1.7119 |
| LSTM | 0.9216 | 1.2611 |
| BiLSTM | 0.9520 | 1.2650 |

LSTM achieves the best performance. BiLSTM performs similarly but slightly worse, while RNN performs noticeably worse due to limited ability to capture long-term dependencies.

<h2>Visualisations</h2>

Figures generated by `train_and_evaluate.py`:

- `actual_vs_predicted.png`  
- `error_distribution.png`

Use:

```
![Actual vs Predicted](figures/actual_vs_predicted.png)
![Error Distribution](figures/error_distribution.png)
```

<h2>How to Run the Project</h2>

<h3>Install dependencies</h3>

```
pip install pandas numpy scikit-learn tensorflow matplotlib
```

<h3>Preprocess the dataset</h3>

```
python data_preprocessing.py
```

<h3>Train and evaluate models</h3>

```
python train_and_evaluate.py
```

<h3>Optional: hyperparameter tuning</h3>

```
python hyperparameter_tuning.py
```

<h2>Discussion</h2>

- LSTM performs best because it captures long-term seasonal patterns and avoids vanishing gradients.  
- RNN struggles with long sequences and provides the highest errors.  
- BiLSTM does not outperform LSTM because forecasting is a **causal** task; backward sequence processing does not provide useful future context.  
- Errors increase during rapid temperature changes or seasonal transitions.

<h2>Future Work</h2>

- Use additional climate features (humidity, cloud cover, etc.)  
- Test GRU, CNN-LSTM, or attention-based models  
- Tune dropout, learning rate, and window sizes  
- Add probabilistic forecasting  

<h2>Dataset Reference</h2>

Hong Kong Observatory — Daily Grass Minimum Temperature  
https://data.gov.hk/en-data/dataset/hk-hko-rss-daily-grass-min-temp
